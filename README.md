# üåéüõ∞Ô∏è MultiMAE meets Earth Observation: Pre-training Multi-modal multi-task Masked Autoencoders for Earth Observation Tasks


## Updates
- **May x, 2025:** Our paper has been acepted for presentation at [ICIP 2025](https://2025.ieeeicip.org/).
- **May x, 2025:** MultiMAE meets Earth Observation paper is released [[arXiv]]()  [[PDF]]().

## Overview
Multi-modal data in Earth Observation (EO) presents a huge opportunity for improving transfer learning capabilities when pre-training deep learning models. Our work proposes a flexible multi-modal, multi-task pre-training strategy for EO data. Specifically, we adopt a Multi-modal Multi-task Masked Autoencoder (MultiMAE) that we pre-train by reconstructing diverse input modalities, including spectral, elevation, and segmentation data. The pre-trained model demonstrates robust transfer learning capabilities, outperforming state-of-the-art methods on various EO datasets for classification and segmentation tasks. Our approach exhibits significant flexibility, handling diverse input configurations without requiring modality-specific pre-trained models.

## Approach

<img width="1096" alt="image" src="images/main_arch.png">

## Pre-trained Models

## Usage

### Pre-training
Code coming soon. Stay tuned ...

### Fine-tuning
Code coming soon. Stay tuned ...


## Acknowledgements
Our work is inspired by [MultiMAE](https://github.com/EPFL-VILAB/MultiMAE) and it uses [MMEarth](https://github.com/vishalned/MMEarth-data) dataset. We thank authors for making their amazing work accesible to the community. 

## Citation
```
@inproceedings{}
```