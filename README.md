# üåéüõ∞Ô∏è MultiMAE meets Earth Observation: Pre-training Multi-modal multi-task Masked Autoencoders for Earth Observation tasks


## Updates
- **May 20, 2025:** MultiMAE meets Earth Observation paper is released [[arXiv]]()  [[PDF]]()

## Overview
Multi-modal data in Earth Observation (EO) presents a huge opportunity for improving transfer learning capabilities when pre-training deep learning models. Our work proposes a flexible multi-modal, multi-task pre-training strategy for EO data. Specifically, we adopt a Multi-modal Multi-task Masked Autoencoder (MultiMAE) that we pre-train by reconstructing diverse input modalities, including spectral, elevation, and segmentation data. The pre-trained model demonstrates robust transfer learning capabilities, outperforming state-of-the-art methods on various EO datasets for classification and segmentation tasks. Our approach exhibits significant flexibility, handling diverse input configurations without requiring modality-specific pre-trained models.

## Approach

<img width="1096" alt="image" src="images/main_arch.png">

## Pre-trained Models

## Usage
# Pre-training
Code coming soon. Stay tuned ....

# Fine-tuning
Code coming soon. Stay tuned ....


## Acknowledgements
The codebase is inspired by [MultiMAE](https://github.com/EPFL-VILAB/MultiMAE). Our work uses [MMEarth](https://github.com/vishalned/MMEarth-data) dataset. We thank authors for making their amazing work accesible to the community. 

## Citation
```
@inproceedings{}
```